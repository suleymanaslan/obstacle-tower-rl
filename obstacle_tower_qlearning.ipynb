{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obstacle_tower_env import ObstacleTowerEnv\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "training_time = int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ObstacleTowerEnv(retro=True, realtime_mode=False)\n",
    "env.seed(550)\n",
    "env.floor(0)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "movement_dict = {0:\"No-Op\", 1:\"Forward\", 2:\"Backward\"}\n",
    "cam_rot_dict = {0:\"No-Op\", 1:\"Counter-Clockwise\", 2:\"Clockwise\"}\n",
    "jump_dict = {0:\"No-Op\", 1:\"Jump\"}\n",
    "turn_dict = {0:\"No-Op\", 1:\"Right\", 2:\"Left\"}\n",
    "\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_mdas(pred_output):\n",
    "    pred_movement = pred_output // 3\n",
    "    pred_cam_rot = pred_output % 3\n",
    "    pred_jump = 0\n",
    "    pred_turn = 0\n",
    "    \n",
    "    pred_action = [pred_movement, pred_cam_rot, pred_jump, pred_turn]\n",
    "    \n",
    "    return np.array(pred_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs_to_md(a_probs):\n",
    "    a_actuals = (a_probs >= 0.5).astype(int)\n",
    "    a_md = [0, 0, 0, 0]\n",
    "    if a_actuals[0] == 1:\n",
    "        if a_actuals[2] == 0:\n",
    "            a_md[1] = 1\n",
    "    if a_actuals[1] == 1:\n",
    "        a_md[0] = 1\n",
    "    if a_actuals[2] == 1:\n",
    "        if a_actuals[0] == 0:\n",
    "            a_md[1] = 2\n",
    "    if a_actuals[3] == 1:\n",
    "        a_md[2] = 1\n",
    "    return a_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_to_d(md_action):\n",
    "    return md_action[0] * 18 + md_action[1] * 6 + md_action[2] * 3 + md_action[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_to_md(d_action):\n",
    "    md_movement = (d_action // 18) \n",
    "    md_cam_rot = (d_action // 6) % 3\n",
    "    md_jump = (d_action // 3) % 2\n",
    "    md_turn = d_action % 3 \n",
    "    \n",
    "    md_action = [md_movement, md_cam_rot, md_jump, md_turn]\n",
    "    \n",
    "    return np.array(md_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in range(1):\n",
    "    random_out = random.randrange(n_actions)\n",
    "    random_action_md = output_to_mdas(random_out)\n",
    "    random_action = md_to_d(random_action_md)\n",
    "    if i == 0:\n",
    "        print(f\"Random action:{random_action_md}\")\n",
    "        print(f\"Movement:{movement_dict[random_action_md[0]]}, RotateCam:{cam_rot_dict[random_action_md[1]]}, \"\n",
    "              f\"Jump:{jump_dict[random_action_md[2]]}, Turn:{turn_dict[random_action_md[3]]}\")\n",
    "    obs, reward, done, info = env.step(random_action)\n",
    "    if done:\n",
    "        print(\"Done\")\n",
    "    counter += 1\n",
    "print(f\"Reward:{reward}, Done:{done}\")\n",
    "plt.imshow(obs)\n",
    "plt.show()\n",
    "\n",
    "counter = 0\n",
    "while not done:\n",
    "    random_action = env.action_space.sample()\n",
    "    random_action_md = d_to_md(random_action)\n",
    "    action_probs = np.array([0.2, 0.9, 0.3, 0.2])\n",
    "    random_action_md = probs_to_md(action_probs)\n",
    "    random_action = md_to_d(random_action_md)\n",
    "    if counter == 0:\n",
    "        print(f\"Random action:{random_action_md}\")\n",
    "        print(f\"Movement:{movement_dict[random_action_md[0]]}, RotateCam:{cam_rot_dict[random_action_md[1]]}, \"\n",
    "              f\"Jump:{jump_dict[random_action_md[2]]}, Turn:{turn_dict[random_action_md[3]]}\")\n",
    "    obs, reward, done, info = env.step(random_action)\n",
    "    if done:\n",
    "        print(\"Done\")\n",
    "    counter += 1\n",
    "print(f\"Reward:{reward}, Done:{done}\")\n",
    "plt.imshow(obs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen(obs):\n",
    "    screen = obs.transpose((2, 0, 1))\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "obs = env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen(obs).cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none')\n",
    "plt.title('Example screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(6, 16, kernel_size=3, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=3, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(32)\n",
    "\n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(conv2d_size_out(w))))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(conv2d_size_out(h))))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 10000\n",
    "TARGET_UPDATE = 4\n",
    "\n",
    "ep_rewards = []\n",
    "\n",
    "AGGREGATE_STATS_EVERY = 10\n",
    "SAVE_MODEL_EVERY = 20\n",
    "\n",
    "init_screen = get_screen(obs)\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(2048)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_state = get_screen(obs)\n",
    "temp_state = torch.cat((temp_state, temp_state), 1)\n",
    "temp_out = select_action(temp_state)\n",
    "random_action_md = output_to_mdas(temp_out.item())\n",
    "random_action = md_to_d(random_action_md)\n",
    "print(f\"Random action:{random_action_md}\")\n",
    "print(f\"Movement:{movement_dict[random_action_md[0]]}, RotateCam:{cam_rot_dict[random_action_md[1]]}, \"\n",
    "      f\"Jump:{jump_dict[random_action_md[2]]}, Turn:{turn_dict[random_action_md[3]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_screen = get_screen(obs)\n",
    "current_screen = get_screen(obs)\n",
    "state = current_screen - last_screen\n",
    "input_state = torch.cat((current_screen, state), 1)\n",
    "for i in range(1): \n",
    "    with torch.no_grad():\n",
    "        action = policy_net(input_state).max(1)[1].view(1, 1)\n",
    "    \n",
    "    obs, reward, done, info = env.step(md_to_d(output_to_mdas(action.item())))\n",
    "    \n",
    "    last_screen = current_screen\n",
    "    current_screen = get_screen(obs)\n",
    "    if not done:\n",
    "        next_state = current_screen - last_screen\n",
    "    else:\n",
    "        next_state = None\n",
    "    \n",
    "    state = next_state\n",
    "    input_state = torch.cat((current_screen, state), 1)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500\n",
    "print(f\"{datetime.now()} Start training\")\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    \n",
    "    episode_reward = 0\n",
    "    \n",
    "    obs = env.reset()\n",
    "    last_screen = get_screen(obs)\n",
    "    current_screen = get_screen(obs)\n",
    "    state = current_screen - last_screen\n",
    "    input_state = torch.cat((current_screen, state), 1)\n",
    "    for t in count():\n",
    "        action = select_action(input_state)\n",
    "        obs, reward, done, info = env.step(md_to_d(output_to_mdas(action.item())))\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen(obs)\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "            next_input_state = torch.cat((current_screen, next_state), 1)\n",
    "        else:\n",
    "            next_state = None\n",
    "            next_input_state = None\n",
    "\n",
    "        memory.push(input_state, action, next_input_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "        input_state = next_input_state\n",
    "    \n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "#             plot_durations()\n",
    "            break\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    print(f\"{datetime.now()} Episode:{i_episode}, EpisodeReward:{episode_reward:.2f}\")\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not i_episode % AGGREGATE_STATS_EVERY or i_episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        print(f\"{datetime.now()} Episode:{i_episode}, AvgReward:{average_reward:.2f}, MinReward:{min_reward:.2f}, MaxReward:{max_reward:.2f}\")\n",
    "    if not i_episode % SAVE_MODEL_EVERY:\n",
    "        if not os.path.exists(f\"models/{training_time}\"):\n",
    "            os.makedirs(f\"models/{training_time}\")\n",
    "        torch.save(policy_net.state_dict(), f\"models/{training_time}/{int(time.time())}_{i_episode}_{average_reward:.2f}_model.pth\")\n",
    "\n",
    "print(f\"{datetime.now()} Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"models/ep_rewards.npy\" , np.array(ep_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_rewards_eval = []\n",
    "print(f\"{datetime.now()} Start evaluation\")\n",
    "for _ in range(20):\n",
    "    done = False\n",
    "    env.seed(np.random.randint(low=560, high=99990))\n",
    "    env.floor(0)\n",
    "    obs = env.reset()\n",
    "    episode_reward = 0\n",
    "    last_screen = get_screen(obs)\n",
    "    current_screen = get_screen(obs)\n",
    "    state = current_screen - last_screen\n",
    "    input_state = torch.cat((current_screen, state), 1)\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = policy_net(input_state).max(1)[1].view(1, 1)\n",
    "\n",
    "        obs, reward, done, info = env.step(md_to_d(output_to_mdas(action.item())))\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen(obs)\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "            next_input_state = torch.cat((current_screen, next_state), 1)\n",
    "        else:\n",
    "            next_state = None\n",
    "            next_input_state = None\n",
    "\n",
    "        state = next_state\n",
    "        input_state = next_input_state\n",
    "        if done:\n",
    "            break\n",
    "    ep_rewards_eval.append(episode_reward)\n",
    "    print(f\"{datetime.now()} Episode reward:{episode_reward:.2f}\")\n",
    "ep_rewards_eval = np.array(ep_rewards_eval)\n",
    "print(f\"{datetime.now()} Min episode reward:{ep_rewards_eval.min()}\")\n",
    "print(f\"{datetime.now()} Max episode reward:{ep_rewards_eval.max()}\")\n",
    "print(f\"{datetime.now()} Mean episode reward:{ep_rewards_eval.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"models/ep_rewards_eval.npy\" , ep_rewards_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
