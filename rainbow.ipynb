{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/Kaixhin/Rainbow\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import atari_py\n",
    "import cv2\n",
    "\n",
    "\n",
    "class Env():\n",
    "    def __init__(self, max_episode_length=1.08e5, history_length=4):\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        self.ale = atari_py.ALEInterface()\n",
    "        self.ale.setInt('max_num_frames_per_episode', max_episode_length)\n",
    "        self.ale.setFloat('repeat_action_probability', 0)\n",
    "        self.ale.setInt('frame_skip', 0)\n",
    "        self.ale.setBool('color_averaging', False)\n",
    "        self.ale.loadROM(atari_py.get_game_path(\"space_invaders\"))\n",
    "        actions = self.ale.getMinimalActionSet()\n",
    "        self.actions = dict([i, e] for i, e in zip(range(len(actions)), actions))\n",
    "        self.window = history_length\n",
    "        self.state_buffer = deque([], maxlen=self.window)\n",
    "        self.training = True\n",
    "    \n",
    "    def _reset_buffer(self):\n",
    "        for _ in range(self.window):\n",
    "            self.state_buffer.append(torch.zeros(84, 84, device=self.device))\n",
    "    \n",
    "    def _get_state(self):\n",
    "        state = cv2.resize(self.ale.getScreenGrayscale(), (84, 84), interpolation=cv2.INTER_LINEAR)\n",
    "        return torch.tensor(state, dtype=torch.float32, device=self.device).div_(255)\n",
    "    \n",
    "    def reset(self):\n",
    "        self._reset_buffer()\n",
    "        self.ale.reset_game()\n",
    "        for _ in range(random.randrange(30)):\n",
    "            self.ale.act(0)\n",
    "            if self.ale.game_over():\n",
    "                self.ale.reset_game()\n",
    "        observation = self._get_state()\n",
    "        self.state_buffer.append(observation)\n",
    "        return torch.stack(list(self.state_buffer), 0)\n",
    "    \n",
    "    def step(self, action):\n",
    "#         print(f\"action:{action}\")\n",
    "        frame_buffer = torch.zeros(2, 84, 84, device=self.device)\n",
    "        reward, done = 0, False\n",
    "        for t in range(4):\n",
    "            reward += self.ale.act(self.actions.get(action))\n",
    "#             print(f\"reward:{reward}\")\n",
    "            if t == 2:\n",
    "                frame_buffer[0] = self._get_state()\n",
    "            elif t == 3:\n",
    "                frame_buffer[1] = self._get_state()\n",
    "            done = self.ale.game_over()\n",
    "            if done:\n",
    "                break\n",
    "        observation = frame_buffer.max(0)[0]\n",
    "#         print(f\"observation:{observation.shape}\")\n",
    "        self.state_buffer.append(observation)\n",
    "        return torch.stack(list(self.state_buffer), 0), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()\n",
    "action_space = len(env.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/Kaixhin/Rainbow\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init=0.5):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.empty(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.empty(out_features))\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n",
    "    \n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        return x.sign().mul_(x.abs().sqrt_())\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            return F.linear(input, self.weight_mu + self.weight_sigma * self.weight_epsilon, self.bias_mu + self.bias_sigma * self.bias_epsilon)\n",
    "        else:\n",
    "            return F.linear(input, self.weight_mu, self.bias_mu)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, atoms, action_space, history_length, hidden_size=256, noisy_std=0.1):\n",
    "        super(DQN, self).__init__()\n",
    "        self.atoms = atoms\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.convs = nn.Sequential(nn.Conv2d(history_length, 32, 5, stride=5, padding=0), nn.ReLU(), \n",
    "                                   nn.Conv2d(32, 64, 5, stride=5, padding=0), nn.ReLU()\n",
    "                                  )\n",
    "        self.conv_output_size = 576\n",
    "        \n",
    "        self.fc_h_v = NoisyLinear(self.conv_output_size, hidden_size, std_init=noisy_std)\n",
    "        self.fc_h_a = NoisyLinear(self.conv_output_size, hidden_size, std_init=noisy_std)\n",
    "        self.fc_z_v = NoisyLinear(hidden_size, self.atoms, std_init=noisy_std)\n",
    "        self.fc_z_a = NoisyLinear(hidden_size, self.action_space * self.atoms, std_init=noisy_std)\n",
    "    \n",
    "    def forward(self, x, use_log_softmax=False):\n",
    "#         print(f\"x:{x.shape}\")\n",
    "        x = self.convs(x)\n",
    "#         print(f\"x:{x.shape}\")\n",
    "        x = x.view(-1, self.conv_output_size)\n",
    "#         print(f\"x:{x.shape}\")\n",
    "        \n",
    "        v = self.fc_z_v(F.relu(self.fc_h_v(x)))\n",
    "#         print(f\"v:{v.shape}\")\n",
    "        a = self.fc_z_a(F.relu(self.fc_h_a(x)))\n",
    "#         print(f\"a:{a.shape}\")\n",
    "        v, a = v.view(-1, 1, self.atoms), a.view(-1, self.action_space, self.atoms)\n",
    "#         print(f\"v:{v.shape}\")\n",
    "#         print(f\"a:{a.shape}\")\n",
    "        q = v + a - a.mean(1, keepdim=True)\n",
    "#         print(f\"q:{q.shape}\")\n",
    "        q = F.log_softmax(q, dim=2) if use_log_softmax else F.softmax(q, dim=2)\n",
    "#         print(f\"q:{q.shape}\")\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        self.fc_h_v.reset_noise()\n",
    "        self.fc_h_a.reset_noise()\n",
    "        self.fc_z_v.reset_noise()\n",
    "        self.fc_z_a.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/Kaixhin/Rainbow\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env, atoms=51, V_min=-10.0, V_max=10.0, batch_size=32, multi_step=20, discount=0.99, norm_clip=10.0, lr=1.0e-4, adam_eps=1.5e-4):\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        self.action_space = len(env.actions)\n",
    "        self.atoms = atoms\n",
    "        self.Vmin = V_min\n",
    "        self.Vmax = V_max\n",
    "        self.support = torch.linspace(self.Vmin, self.Vmax, self.atoms).to(self.device)\n",
    "        self.delta_z = (self.Vmax - self.Vmin) / (self.atoms - 1)\n",
    "        self.batch_size = batch_size\n",
    "        self.n = multi_step\n",
    "        self.discount = discount\n",
    "        self.norm_clip = norm_clip\n",
    "        \n",
    "        self.online_net = DQN(self.atoms, self.action_space, env.window).to(self.device)\n",
    "        self.online_net.train()\n",
    "        \n",
    "        self.target_net = DQN(self.atoms, self.action_space, env.window).to(self.device)\n",
    "        self.update_target_net()\n",
    "        self.target_net.train()\n",
    "        \n",
    "        for param in self.target_net.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.online_net.parameters(), lr=lr, eps=adam_eps)\n",
    "        \n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        \n",
    "    def reset_noise(self):\n",
    "        self.online_net.reset_noise()\n",
    "        \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            return (self.online_net(state.unsqueeze(0)) * self.support).sum(2).argmax(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/Kaixhin/Rainbow\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition', ('timestep', 'state', 'action', 'reward', 'nonterminal'))\n",
    "\n",
    "\n",
    "class SegmentTree():\n",
    "    def __init__(self, size):\n",
    "        self.index = 0\n",
    "        self.size = size\n",
    "        self.full = False\n",
    "        self.sum_tree = np.zeros((2 * size - 1, ), dtype=np.float32)\n",
    "        self.data = np.array([None] * size)\n",
    "        self.max = 1\n",
    "    \n",
    "    def _propagate(self, index, value):\n",
    "        parent = (index - 1) // 2\n",
    "        left, right = 2 * parent + 1, 2 * parent + 2\n",
    "        self.sum_tree[parent] = self.sum_tree[left] + self.sum_tree[right]\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, value)\n",
    "    \n",
    "    def update(self, index, value):\n",
    "        self.sum_tree[index] = value\n",
    "        self._propagate(index, value)\n",
    "        self.max = max(value, self.max)\n",
    "    \n",
    "    def append(self, data, value):\n",
    "        self.data[self.index] = data\n",
    "        self.update(self.index + self.size - 1, value)\n",
    "        self.index = (self.index + 1) % self.size\n",
    "        self.full = self.full or self.index == 0\n",
    "        self.max = max(value, self.max)\n",
    "\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity, history_length, discount, multi_step, priority_weight=0.4, priority_exponent=0.5):\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        self.capacity = capacity\n",
    "        self.history = history_length\n",
    "        self.discount = discount\n",
    "        self.n = multi_step\n",
    "        self.priority_weight = priority_weight\n",
    "        self.priority_exponent = priority_exponent\n",
    "        self.t = 0\n",
    "        self.transitions = SegmentTree(capacity)\n",
    "        \n",
    "    def append(self, state, action, reward, terminal):\n",
    "        state = state[-1].mul(255).to(dtype=torch.uint8, device=torch.device(\"cpu\"))\n",
    "        self.transitions.append(Transition(self.t, state, action, reward, not terminal), self.transitions.max)\n",
    "        self.t = 0 if terminal else self.t + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = ReplayMemory(100_000, env.window, agent.discount, agent.n)\n",
    "T_max = 100_000\n",
    "learn_start = 1_600\n",
    "priority_weight_increase = (1 - mem.priority_weight) / (T_max - learn_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(env._get_state().cpu().numpy(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "state, done = env.reset(), False\n",
    "plt.figure()\n",
    "plt.imshow(state[-1].cpu().numpy(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "action = agent.act(state)\n",
    "state, reward, done = env.step(action)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(state[-1].cpu().numpy(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_clip = 1\n",
    "\n",
    "T, done = 0, True\n",
    "replay_frequency = 1\n",
    "for T in trange(1, T_max + 1):\n",
    "    if done:\n",
    "        state, done = env.reset(), False\n",
    "\n",
    "    if T % replay_frequency == 0:\n",
    "        agent.reset_noise()\n",
    "\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    if reward_clip > 0:\n",
    "        reward = max(min(reward, reward_clip), -reward_clip)\n",
    "    mem.append(state, action, reward, done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
