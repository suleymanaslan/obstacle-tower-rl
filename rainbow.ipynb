{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/Kaixhin/Rainbow\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import cv2\n",
    "import gym\n",
    "from gym.wrappers.pixel_observation import PixelObservationWrapper\n",
    "\n",
    "\n",
    "class Env():\n",
    "    def __init__(self, action_size=4, history_length=4):\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        self.wrapped_env = PixelObservationWrapper(gym.make(\"LunarLander-v2\"), pixels_only=True)\n",
    "        self.action_space = [i for i in range(action_size)]\n",
    "        self.window = history_length\n",
    "        self.state_buffer = deque([], maxlen=self.window)\n",
    "    \n",
    "    def _reset_buffer(self):\n",
    "        for _ in range(self.window):\n",
    "            self.state_buffer.append(torch.zeros(84, 84, device=self.device))\n",
    "            \n",
    "    def _process_observation(self, observation):\n",
    "        observation = cv2.cvtColor(cv2.resize(observation[\"pixels\"], (84, 84), interpolation=cv2.INTER_AREA), cv2.COLOR_RGB2GRAY)\n",
    "        observation = torch.tensor(observation, dtype=torch.float32, device=self.device).div_(255)\n",
    "        self.state_buffer.append(observation)\n",
    "        return torch.stack(list(self.state_buffer), 0)\n",
    "    \n",
    "    def reset(self):\n",
    "        self._reset_buffer()\n",
    "        observation = self.wrapped_env.reset()\n",
    "        observation = self._process_observation(observation)\n",
    "        return observation\n",
    "    \n",
    "    def close(self):\n",
    "        self.wrapped_env.close()\n",
    "    \n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.wrapped_env.step(action)\n",
    "        observation = self._process_observation(observation)\n",
    "        return observation, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/Kaixhin/Rainbow\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init=0.5):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.empty(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.empty(out_features))\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n",
    "    \n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        return x.sign().mul_(x.abs().sqrt_())\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            return F.linear(input, self.weight_mu + self.weight_sigma * self.weight_epsilon, self.bias_mu + self.bias_sigma * self.bias_epsilon)\n",
    "        else:\n",
    "            return F.linear(input, self.weight_mu, self.bias_mu)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, atoms, action_size, history_length, hidden_size=256, noisy_std=0.1):\n",
    "        super(DQN, self).__init__()\n",
    "        self.atoms = atoms\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.convs = nn.Sequential(nn.Conv2d(history_length, 32, 5, stride=5, padding=0), nn.ReLU(), \n",
    "                                   nn.Conv2d(32, 64, 5, stride=5, padding=0), nn.ReLU()\n",
    "                                  )\n",
    "        self.conv_output_size = 576\n",
    "        \n",
    "        self.fc_h_v = NoisyLinear(self.conv_output_size, hidden_size, std_init=noisy_std)\n",
    "        self.fc_h_a = NoisyLinear(self.conv_output_size, hidden_size, std_init=noisy_std)\n",
    "        self.fc_z_v = NoisyLinear(hidden_size, self.atoms, std_init=noisy_std)\n",
    "        self.fc_z_a = NoisyLinear(hidden_size, self.action_size * self.atoms, std_init=noisy_std)\n",
    "    \n",
    "    def forward(self, x, use_log_softmax=False):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self.conv_output_size)\n",
    "        \n",
    "        v = self.fc_z_v(F.relu(self.fc_h_v(x)))\n",
    "        a = self.fc_z_a(F.relu(self.fc_h_a(x)))\n",
    "        v, a = v.view(-1, 1, self.atoms), a.view(-1, self.action_size, self.atoms)\n",
    "        q = v + a - a.mean(1, keepdim=True)\n",
    "        q = F.log_softmax(q, dim=2) if use_log_softmax else F.softmax(q, dim=2)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        self.fc_h_v.reset_noise()\n",
    "        self.fc_h_a.reset_noise()\n",
    "        self.fc_z_v.reset_noise()\n",
    "        self.fc_z_a.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/Kaixhin/Rainbow\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env, atoms=51, V_min=-10.0, V_max=10.0, batch_size=32, multi_step=20, discount=0.99, norm_clip=10.0, lr=1.0e-4, adam_eps=1.5e-4):\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        self.action_size = len(env.action_space)\n",
    "        self.atoms = atoms\n",
    "        self.Vmin = V_min\n",
    "        self.Vmax = V_max\n",
    "        self.support = torch.linspace(self.Vmin, self.Vmax, self.atoms).to(self.device)\n",
    "        self.delta_z = (self.Vmax - self.Vmin) / (self.atoms - 1)\n",
    "        self.batch_size = batch_size\n",
    "        self.n = multi_step\n",
    "        self.discount = discount\n",
    "        self.norm_clip = norm_clip\n",
    "        \n",
    "        self.online_net = DQN(self.atoms, self.action_size, env.window).to(self.device)\n",
    "        self.online_net.train()\n",
    "        \n",
    "        self.target_net = DQN(self.atoms, self.action_size, env.window).to(self.device)\n",
    "        self.update_target_net()\n",
    "        self.target_net.train()\n",
    "        \n",
    "        for param in self.target_net.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.online_net.parameters(), lr=lr, eps=adam_eps)\n",
    "    \n",
    "    def train(self):\n",
    "        self.online_net.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.online_net.eval()\n",
    "    \n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        self.online_net.reset_noise()\n",
    "    \n",
    "    def act(self, observation):\n",
    "        with torch.no_grad():\n",
    "            return (self.online_net(observation.unsqueeze(0)) * self.support).sum(2).argmax(1).item()\n",
    "    \n",
    "    def act_e_greedy(self, state, epsilon=0.001):\n",
    "        return np.random.randint(0, self.action_size) if np.random.random() < epsilon else self.act(state)\n",
    "    \n",
    "    def learn(self, mem):\n",
    "        idxs, states, actions, returns, next_states, nonterminals, weights = mem.sample(self.batch_size)\n",
    "        \n",
    "        log_ps = self.online_net(states, use_log_softmax=True)\n",
    "        log_ps_a = log_ps[range(self.batch_size), actions]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pns = self.online_net(next_states)\n",
    "            dns = self.support.expand_as(pns) * pns\n",
    "            argmax_indices_ns = dns.sum(2).argmax(1)\n",
    "            self.target_net.reset_noise()\n",
    "            pns = self.target_net(next_states)\n",
    "            pns_a = pns[range(self.batch_size), argmax_indices_ns]\n",
    "            \n",
    "            Tz = returns.unsqueeze(1) + nonterminals * (self.discount ** self.n) * self.support.unsqueeze(0)\n",
    "            Tz = Tz.clamp(min=self.Vmin, max=self.Vmax)\n",
    "            b = (Tz - self.Vmin) / self.delta_z\n",
    "            l, u = b.floor().to(torch.int64), b.ceil().to(torch.int64)\n",
    "            l[(u > 0) * (l == u)] -= 1\n",
    "            u[(l < (self.atoms - 1)) * (l == u)] += 1\n",
    "            \n",
    "            m = states.new_zeros(self.batch_size, self.atoms)\n",
    "            offset = torch.linspace(0, ((self.batch_size - 1) * self.atoms), self.batch_size).unsqueeze(1).expand(self.batch_size, self.atoms).to(actions)\n",
    "            m.view(-1).index_add_(0, (l + offset).view(-1), (pns_a * (u.float() - b)).view(-1))\n",
    "            m.view(-1).index_add_(0, (u + offset).view(-1), (pns_a * (b - l.float())).view(-1))\n",
    "        \n",
    "        loss = -torch.sum(m * log_ps_a, 1)\n",
    "        self.online_net.zero_grad()\n",
    "        (weights * loss).mean().backward()\n",
    "        clip_grad_norm_(self.online_net.parameters(), self.norm_clip)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        mem.update_priorities(idxs, loss.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/Kaixhin/Rainbow\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition', ('timestep', 'state', 'action', 'reward', 'nonterminal'))\n",
    "blank_trans = Transition(0, torch.zeros(84, 84, dtype=torch.uint8), None, 0, False)\n",
    "\n",
    "\n",
    "class SegmentTree():\n",
    "    def __init__(self, size):\n",
    "        self.index = 0\n",
    "        self.size = size\n",
    "        self.full = False\n",
    "        self.sum_tree = np.zeros((2 * size - 1, ), dtype=np.float32)\n",
    "        self.data = np.array([None] * size)\n",
    "        self.max = 1\n",
    "    \n",
    "    def _propagate(self, index, value):\n",
    "        parent = (index - 1) // 2\n",
    "        left, right = 2 * parent + 1, 2 * parent + 2\n",
    "        self.sum_tree[parent] = self.sum_tree[left] + self.sum_tree[right]\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, value)\n",
    "    \n",
    "    def update(self, index, value):\n",
    "        self.sum_tree[index] = value\n",
    "        self._propagate(index, value)\n",
    "        self.max = max(value, self.max)\n",
    "    \n",
    "    def append(self, data, value):\n",
    "        self.data[self.index] = data\n",
    "        self.update(self.index + self.size - 1, value)\n",
    "        self.index = (self.index + 1) % self.size\n",
    "        self.full = self.full or self.index == 0\n",
    "        self.max = max(value, self.max)\n",
    "        \n",
    "    def total(self):\n",
    "        return self.sum_tree[0]\n",
    "    \n",
    "    def _retrieve(self, index, value):\n",
    "        left, right = 2 * index + 1, 2 * index + 2\n",
    "        if left >= len(self.sum_tree):\n",
    "            return index\n",
    "        elif value <= self.sum_tree[left]:\n",
    "            return self._retrieve(left, value)\n",
    "        else:\n",
    "            return self._retrieve(right, value - self.sum_tree[left])\n",
    "    \n",
    "    def find(self, value):\n",
    "        index = self._retrieve(0, value)\n",
    "        data_index = index - self.size + 1\n",
    "        return (self.sum_tree[index], data_index, index)\n",
    "    \n",
    "    def get(self, data_index):\n",
    "        return self.data[data_index % self.size]\n",
    "\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity, history_length, discount, multi_step, priority_weight=0.4, priority_exponent=0.5):\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        self.capacity = capacity\n",
    "        self.history = history_length\n",
    "        self.discount = discount\n",
    "        self.n = multi_step\n",
    "        self.priority_weight = priority_weight\n",
    "        self.priority_exponent = priority_exponent\n",
    "        self.t = 0\n",
    "        self.transitions = SegmentTree(capacity)\n",
    "        \n",
    "    def append(self, state, action, reward, terminal):\n",
    "        state = state[-1].mul(255).to(dtype=torch.uint8, device=torch.device(\"cpu\"))\n",
    "        self.transitions.append(Transition(self.t, state, action, reward, not terminal), self.transitions.max)\n",
    "        self.t = 0 if terminal else self.t + 1\n",
    "        \n",
    "    def _get_transition(self, idx):\n",
    "        transition = np.array([None] * (self.history + self.n))\n",
    "        transition[self.history - 1] = self.transitions.get(idx)\n",
    "        for t in range(self.history - 2, -1, -1):\n",
    "            if transition[t + 1].timestep == 0:\n",
    "                transition[t] = blank_trans\n",
    "            else:\n",
    "                transition[t] = self.transitions.get(idx - self.history + 1 + t)\n",
    "        for t in range(self.history, self.history + self.n):\n",
    "            if transition[t - 1].nonterminal:\n",
    "                transition[t] = self.transitions.get(idx - self.history + 1 + t)\n",
    "            else:\n",
    "                transition[t] = blank_trans\n",
    "        return transition\n",
    "        \n",
    "    def _get_sample_from_segment(self, segment, i):\n",
    "        valid = False\n",
    "        while not valid:\n",
    "            sample = np.random.uniform(i * segment, (i + 1) * segment)\n",
    "            prob, idx, tree_idx = self.transitions.find(sample)\n",
    "            if (self.transitions.index - idx) % self.capacity > self.n and (idx - self.transitions.index) % self.capacity >= self.history and prob != 0:\n",
    "                valid = True\n",
    "        \n",
    "        transition = self._get_transition(idx)\n",
    "        state = torch.stack([trans.state for trans in transition[:self.history]]).to(device=self.device).to(dtype=torch.float32).div_(255)\n",
    "        next_state = torch.stack([trans.state for trans in transition[self.n:self.n + self.history]]).to(device=self.device).to(dtype=torch.float32).div_(255)\n",
    "        action = torch.tensor([transition[self.history - 1].action], dtype=torch.int64, device=self.device)\n",
    "        R = torch.tensor([sum(self.discount ** n * transition[self.history + n - 1].reward for n in range(self.n))], dtype=torch.float32, device=self.device)\n",
    "        nonterminal = torch.tensor([transition[self.history + self.n - 1].nonterminal], dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        return prob, idx, tree_idx, state, action, R, next_state, nonterminal\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        p_total = self.transitions.total()\n",
    "        segment = p_total / batch_size\n",
    "        batch = [self._get_sample_from_segment(segment, i) for i in range(batch_size)]\n",
    "        probs, idxs, tree_idxs, states, actions, returns, next_states, nonterminals = zip(*batch)\n",
    "        states, next_states, = torch.stack(states), torch.stack(next_states)\n",
    "        actions, returns, nonterminals = torch.cat(actions), torch.cat(returns), torch.stack(nonterminals)\n",
    "        probs = np.array(probs, dtype=np.float32) / p_total\n",
    "        capacity = self.capacity if self.transitions.full else self.transitions.index\n",
    "        weights = (capacity * probs) ** -self.priority_weight\n",
    "        weights = torch.tensor(weights / weights.max(), dtype=torch.float32, device=self.device)\n",
    "        return tree_idxs, states, actions, returns, next_states, nonterminals, weights\n",
    "    \n",
    "    def update_priorities(self, idxs, priorities):\n",
    "        priorities = np.power(priorities, self.priority_exponent)\n",
    "        [self.transitions.update(idx, priority) for idx, priority in zip(idxs, priorities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "env = Env(history_length=10)\n",
    "agent = Agent(env)\n",
    "mem = ReplayMemory(100_000, env.window, agent.discount, agent.n)\n",
    "\n",
    "episodes = 3\n",
    "replay_frequency = 1\n",
    "reward_clip = 1\n",
    "max_steps = 200_000\n",
    "learning_start_step = 1600\n",
    "priority_weight_increase = (1 - mem.priority_weight) / (max_steps - learning_start_step)\n",
    "\n",
    "rewards = []\n",
    "ep_rewards = []\n",
    "\n",
    "print(f\"{datetime.now()}, start training\")\n",
    "steps = 0\n",
    "for episode_ix in range(1, episodes+1):\n",
    "    observation, ep_reward, ep_steps, done = env.reset(), 0, 0, False\n",
    "#     while not done:\n",
    "    for _ in range(3):\n",
    "        if steps % replay_frequency == 0:\n",
    "            agent.reset_noise()\n",
    "        action = agent.act(observation)\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        ep_steps += 1\n",
    "        steps += 1\n",
    "        print(f\"{datetime.now()}, episode:{episode_ix:2d}, step:{steps:5d}, action:{action}, reward:{reward:9.4f}, done:{done}\")\n",
    "        if reward_clip > 0:\n",
    "            reward = max(min(reward, reward_clip), -reward_clip)\n",
    "        mem.append(observation, action, reward, done)\n",
    "        if steps >= learning_start_step:\n",
    "            mem.priority_weight = min(mem.priority_weight + priority_weight_increase, 1)\n",
    "            if steps % replay_frequency == 0:\n",
    "                pass\n",
    "#                 agent.learn(mem)\n",
    "            \n",
    "        observation = next_observation\n",
    "    ep_rewards.append(ep_reward)\n",
    "    print(f\"{datetime.now()}, episode:{episode_ix:2d}, step:{steps:5d}, reward:{ep_reward:10.4f}\")\n",
    "print(f\"{datetime.now()}, end training\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_clip = 1\n",
    "target_update = 2000\n",
    "\n",
    "rewards = []\n",
    "eval_rewards = []\n",
    "episode_rewards = []\n",
    "eval_episode_rewards = []\n",
    "episode_reward = 0.0\n",
    "episode_steps = 0\n",
    "episode_count = 0\n",
    "\n",
    "done = True\n",
    "for T in range(1, T_max + 1):\n",
    "    if done:\n",
    "        state, done = env.reset(), False\n",
    "\n",
    "    if T % replay_frequency == 0:\n",
    "        agent.reset_noise()\n",
    "\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    episode_reward += reward\n",
    "    episode_steps += 1\n",
    "    if done:\n",
    "        episode_count += 1\n",
    "        episode_rewards.append(episode_reward)\n",
    "        print(f\"{datetime.now()}, T:{T}, Episode:{episode_count}, Steps:{episode_steps}, Avg. Reward:{episode_reward/episode_steps:.4f}, Total Reward:{episode_reward}\")\n",
    "        episode_reward = 0.0\n",
    "        episode_steps = 0\n",
    "    \n",
    "    if reward_clip > 0:\n",
    "        reward = max(min(reward, reward_clip), -reward_clip)\n",
    "    mem.append(state, action, reward, done)\n",
    "    \n",
    "    if T >= learn_start:\n",
    "        mem.priority_weight = min(mem.priority_weight + priority_weight_increase, 1)\n",
    "        \n",
    "        if T % replay_frequency == 0:\n",
    "            agent.learn(mem)\n",
    "            \n",
    "        if episode_count % 10 == 0 and done:\n",
    "            agent.eval()\n",
    "            \n",
    "            eval_episode_reward = 0.0\n",
    "            eval_episode_steps = 0\n",
    "            eval_done = True\n",
    "            while True:\n",
    "                if eval_done:\n",
    "                    eval_state, eval_done = env.reset(), False\n",
    "                eval_action = agent.act_e_greedy(eval_state)\n",
    "                eval_state, eval_reward, eval_done = env.step(eval_action)\n",
    "                eval_rewards.append(eval_reward)\n",
    "                eval_episode_reward += eval_reward\n",
    "                eval_episode_steps += 1\n",
    "                if eval_done:\n",
    "                    eval_episode_rewards.append(eval_episode_reward)\n",
    "                    print(f\"{datetime.now()}, T:{T}, Eval_Episode:{episode_count}, Steps:{eval_episode_steps}, \"\n",
    "                          f\"Avg. Reward:{eval_episode_reward/eval_episode_steps:.4f}, Total Reward:{eval_episode_reward}\")\n",
    "                    break\n",
    "            agent.train()\n",
    "        \n",
    "        if T % target_update == 0:\n",
    "            agent.update_target_net()\n",
    "    \n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,500])\n",
    "plt.hist(rewards)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(episode_rewards)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(eval_episode_rewards)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
